"""
Test Script to Verify All Improvements
Tests ML models, train/test separation, and realistic data generation
"""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from simulation.environment import HybridWorkEnvironment
from simulation.experiment import ExperimentRunner
from core.ai_engine import BehavioralAnalyticsModel, AIAnomalyDetector
from data_loader import RealWorldDataLoader
import config


def test_ml_model():
    """Test that ML model works correctly"""
    print("\n" + "="*70)
    print("TEST 1: ML Model Functionality")
    print("="*70)
    
    try:
        model = BehavioralAnalyticsModel()
        print("âœ“ BehavioralAnalyticsModel initialized")
        
        # Create sample training data
        import datetime
        behavior_history = []
        base_time = datetime.datetime.now() - datetime.timedelta(days=30)
        
        for i in range(50):
            behavior_history.append({
                'timestamp': base_time + datetime.timedelta(hours=i*2),
                'hour': (9 + i % 8),  # Work hours
                'day_of_week': i % 7,
                'resource': f'resource_{i % 5}',
                'location': f'location_{i % 3}',
                'device_id': f'device_{i % 2}',
                'date': str((base_time + datetime.timedelta(hours=i*2)).date()),
                'success': True,
                'failed_auth': False
            })
        
        # Train model
        model.train_user_model('test_user', behavior_history)
        print(f"âœ“ Model trained on {len(behavior_history)} samples")
        
        # Test prediction
        current_behavior = {
            'hour': 3,  # Unusual hour (3 AM)
            'resource': 'unusual_resource',
            'location': 'unusual_location',
            'access_rate': 50,  # High access rate
            'location_changes': 5
        }
        
        anomaly_score = model.predict_anomaly_score('test_user', current_behavior)
        print(f"âœ“ Anomaly prediction works: score = {anomaly_score:.3f}")
        
        # Check model statistics
        stats = model.get_model_statistics('test_user')
        print(f"âœ“ Model statistics: {stats}")
        
        return True
        
    except Exception as e:
        print(f"âœ— ML Model test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_train_test_separation():
    """Test that train/test separation works"""
    print("\n" + "="*70)
    print("TEST 2: Train/Test Separation (Circular Logic Fix)")
    print("="*70)
    
    try:
        env = HybridWorkEnvironment(use_realistic_generation=True)
        env.setup_environment()
        
        # Set training phase
        training_days = 7
        env.set_training_phase(True, training_days)
        env.identity_manager.set_training_phase(True)
        print(f"âœ“ Training phase set: Days 1-{training_days}")
        
        # Simulate training days
        print("  Simulating training days...")
        for day in range(1, training_days + 1):
            env.simulate_day(day)
        
        # Check that models were trained
        trained_users = len([uid for uid in env.identity_manager.users.keys() 
                           if uid in env.identity_manager.ai_detector.behavioral_model.user_models])
        print(f"âœ“ Models trained for {trained_users} users during training phase")
        
        # Switch to testing phase
        env.set_training_phase(False, training_days)
        env.identity_manager.set_training_phase(False)
        print(f"âœ“ Testing phase set: Days {training_days+1}-{training_days+7}")
        
        # Simulate test days
        print("  Simulating test days (models should be frozen)...")
        initial_model_count = len(env.identity_manager.ai_detector.behavioral_model.user_models)
        
        for day in range(training_days + 1, training_days + 4):  # Just 3 test days
            env.simulate_day(day)
        
        # Verify models weren't retrained (count should be same or less, not more)
        final_model_count = len(env.identity_manager.ai_detector.behavioral_model.user_models)
        print(f"âœ“ Model count before test: {initial_model_count}, after test: {final_model_count}")
        print("âœ“ Models frozen during testing phase (no retraining)")
        
        return True
        
    except Exception as e:
        print(f"âœ— Train/Test separation test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_realistic_generation():
    """Test realistic behavior generation"""
    print("\n" + "="*70)
    print("TEST 3: Realistic Data Generation")
    print("="*70)
    
    try:
        env = HybridWorkEnvironment(use_realistic_generation=True)
        env.setup_environment()
        
        print("âœ“ Environment setup with realistic generation")
        
        # Check that behavior generator exists
        if env.behavior_generator:
            print("âœ“ RealisticBehaviorGenerator initialized")
            
            # Check user patterns
            user = env.users[0]
            if user.user_id in env.behavior_generator.user_patterns:
                pattern = env.behavior_generator.user_patterns[user.user_id]
                print(f"âœ“ User patterns created for {user.user_id}:")
                print(f"    Work hours: {pattern['work_start_hour']}:00 - {pattern['work_end_hour']}:00")
                print(f"    Preferred resources: {pattern['preferred_resources'][:3]}")
                print(f"    Primary device: {pattern['primary_device']}")
        else:
            print("âš  User patterns not yet initialized (will be created during simulation)")
        
        # Simulate a day and check patterns
        print("\n  Simulating one day to verify realistic patterns...")
        env.simulate_day(1)
        
        # Check that events were generated with realistic patterns
        auth_logs = env.identity_manager.authentication_logs
        if auth_logs:
            print(f"âœ“ Generated {len(auth_logs)} authentication events")
            
            # Check time distribution (should favor work hours)
            from collections import Counter
            hours = [log['timestamp'].hour for log in auth_logs if isinstance(log.get('timestamp'), type)]
            if hours:
                hour_dist = Counter(hours)
                work_hour_events = sum(count for hour, count in hour_dist.items() if 9 <= hour <= 17)
                total_events = sum(hour_dist.values())
                work_hour_ratio = work_hour_events / total_events if total_events > 0 else 0
                print(f"âœ“ Work hour (9-17) event ratio: {work_hour_ratio:.1%} (should be >50%)")
        
        return True
        
    except Exception as e:
        print(f"âœ— Realistic generation test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_data_loader():
    """Test real-world data loader"""
    print("\n" + "="*70)
    print("TEST 4: Real-World Data Loader")
    print("="*70)
    
    try:
        loader = RealWorldDataLoader()
        print("âœ“ RealWorldDataLoader initialized")
        
        # Test dataset info
        info = loader.download_sample_datasets_info()
        print("âœ“ Dataset information available")
        
        # Test loading (will show message if no data found, which is expected)
        train_data, test_data = loader.load_dataset('nonexistent.csv', format_type='csv')
        print("âœ“ Data loader handles missing files gracefully")
        
        # Check structure
        if train_data == [] and test_data == []:
            print("âœ“ Returns empty lists when dataset not found (expected)")
        
        return True
        
    except Exception as e:
        print(f"âœ— Data loader test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_full_experiment():
    """Test a mini experiment to verify everything works together"""
    print("\n" + "="*70)
    print("TEST 5: Full Experiment Integration")
    print("="*70)
    
    try:
        # Temporarily reduce simulation days for quick test
        original_days = config.SIMULATION_DAYS
        config.SIMULATION_DAYS = 10  # Just 10 days for quick test
        
        print("Running mini experiment (10 days, 7 training + 3 testing)...")
        
        runner = ExperimentRunner(experiment_name="test_improvements")
        
        # Run just baseline scenario (quicker)
        print("\n  Running baseline scenario...")
        baseline = runner.run_baseline_scenario(train_test_split=0.7)
        
        if baseline:
            print("âœ“ Baseline scenario completed")
            print(f"  - Breach prevention rate: {baseline['breach_prevention']['prevention_rate']:.1%}")
            print(f"  - Security score: {baseline['security']['security_score']:.1f}")
        
        # Restore original config
        config.SIMULATION_DAYS = original_days
        
        return True
        
    except Exception as e:
        print(f"âœ— Full experiment test failed: {e}")
        import traceback
        traceback.print_exc()
        config.SIMULATION_DAYS = original_days  # Restore anyway
        return False


def main():
    """Run all tests"""
    print("\n" + "="*70)
    print("TESTING ALL IMPROVEMENTS")
    print("="*70)
    print("\nThis script verifies:")
    print("  1. ML Model (Isolation Forest) works")
    print("  2. Train/Test separation (no circular logic)")
    print("  3. Realistic data generation (time/role/sequence-based)")
    print("  4. Real-world data loader")
    print("  5. Full experiment integration")
    
    results = []
    
    # Run tests
    results.append(("ML Model", test_ml_model()))
    results.append(("Train/Test Separation", test_train_test_separation()))
    results.append(("Realistic Generation", test_realistic_generation()))
    results.append(("Data Loader", test_data_loader()))
    results.append(("Full Experiment", test_full_experiment()))
    
    # Summary
    print("\n" + "="*70)
    print("TEST SUMMARY")
    print("="*70)
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for test_name, result in results:
        status = "âœ“ PASS" if result else "âœ— FAIL"
        print(f"{status}: {test_name}")
    
    print(f"\nTotal: {passed}/{total} tests passed")
    
    if passed == total:
        print("\nðŸŽ‰ All improvements verified successfully!")
    else:
        print(f"\nâš  {total - passed} test(s) failed. Check errors above.")
    
    return passed == total


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)

